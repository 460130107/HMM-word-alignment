\documentclass{article}
\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{verbatim}

\begin{comment}
    TODOs and stuff
    Makefile experiment runs
    clean and doc py scripts
    charts and tables
    Makefile to package and publish
    make and push a bibtex
    corpus examples
\end{comment}

\title{Excursions in Word Alignment: Model Initialization and Symmetrization}
% dalliances
% forrays
% expeditions
% other whimsical and self- and task-deprecating synonyms
\author{Adithya Renduchintala and Aric Velbel}

\begin{document}

\maketitle

\quote{
We compare the AER performance of IBM Models 1 and 2 using a couple of initialization methods with an HMM alignment model incorporating some heuristics for speed advantage. We demonstrate the effects of various symmetrization methods combining English-French and French-English alignments generated by all of these models, and discuss the efficacy of AER as a useful and informative metric and the alignment task in general.
}

\section{Initializing IBM Models}

Given sets of English and French sentences $E_1^M$ and $F_1^N$, for every French token $f$, we designate an initial set $S_f$ of possible candidate translations as

\[
    S_f = \bigcup_{i=1}^N E_i \text{if} f \in F_i
\]

Under the naive scheme, probability is distributed among these candidates uniformly:

\[
    p(f|e) = \frac{1}{|S_f|}
\]

A more nuanced approach obtains initial weights by exploiting the string-level commonalities between English and French words that arise due to their languages' linguistic proximity. Where ld is the Levenshtein distance function, we take the {\em edit ratio} between to strings $a$ and $b$ to be 
\[
    \text{er}(a,b) = \frac{\text{ld}(a, b)}{\max(|a|, |b|)} + \delta
\]
and redefine the initial translation probabilities as
\[
    p(f|e) = \frac{\text{er}(f,e)}{\sum_{i=1}^{|S_f|} \text{er}(f, {S_f}_i)}
\]
The smoothing term $\delta$ TODO. Results are shown in Table \ref{tbl:ed_dist}.
 %todo more explain
% TODO cite msr paper as inspiration?

    \begin{table}[h]
\begin{center}
\begin{tabular}{l|ll}
    & \multicolumn{2}{c} {AER}\\
    initialization & Model 1 & Model 2\\ \hline
    uniform & & \\
    edit distance & &
\end{tabular}
\end{center}
\label{tbl:ed_dist}
\caption{Using a function of Levenshtein edit distance to seed Model 1 and Model 2 improves AER over the baseline uniform initialization.}
\end{table}

\section{An HMM alignment model}

% rephrase this to be more specific
To test our intuition that local context ought to matter for alignment, we implemented a Hidden Markov Model-based alignment scheme following the work of \cite{todo}.

{\tt hmm-model.py}

In order to make the runtime of these experiments more manageable, we implemented a heuristic pruning TODO

% TODO cite inspiration paper

Figure \ref{fig:hmm_learning_curve} shows the improvement in the HMM aligner's performance over several iterations.

\begin{figure}
\begin{center}
\end{center}
\label{fig:hmm_learning_curve}
\end{figure}

\section{Symmetrization of forward and reverse alignments}

Because many-to-one alignments are restricted to the F $\rightarrow$ E direction in the first two IBM models, we decided to try combining the output of the aligner when run in each direction in turn. One might hope that this process would go some way toward accomodating phrasal translations with a ``fertility" of less than 1, for example when an English idiom, phrase, or functional n-gram translates concisely as a single French word. We see some examples of these phenomena in the hand-annontated Hansards examples in Figure \ref{fig:etof}.

\begin{figure}
\begin{center}
\end{center}
\label{fig:etof}
\end{figure}

A simple approach to this task is described in \cite{koehn}, which we implement in ({\tt merge.py}). Contrary to expectation, symmetrizing forward and reverse alignments in this way did not improve our measured alignment performance. However, simply taking the intersection of the forward and reverse alignments did improve our score. This result furthers our skeptecism of the comprehensiveness and utility of the AER metric, as we discuss further in \ref{aer}.
% more detail about this?

Table \ref{tbl:sym} shows compares AER for the fully symmetrized, intersected, and unsymmetrized output of our experimental runs.

\section{AER and the Hansards alignments}
\label{aer}

\section{Summary of results and analysis}

\begin{table}
\begin{center}
\begin{tabular}{r|cccc}
    & $p(f|e)$ & $p(e|f)$ & sym & int\\ \hline
    Model 1 & & & & \\
    % diff inits?
    Model 2 & & & & \\
    HMM - min iter & & & & \\
    HMM - max iter & & & & \\

\end{tabular}
\end{center}
\end{table}


ablation table



\end{document}
