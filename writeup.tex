\documentclass{article}
\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{verbatim}
\usepackage{cite}

\begin{comment}
    TODOs and stuff
    Makefile experiment runs
    clean and doc py scripts
    charts and tables
    Makefile to package and publish
    make and push a bibtex
    corpus examples
\end{comment}

\title{Excursions in Word Alignment: Model Initialization and Symmetrization}
% dalliances
% forrays
% expeditions
% other whimsical and self- and task-deprecating synonyms
\author{Adithya Renduchintala and Aric Velbel}

\begin{document}

\maketitle

\quote{
We compare the AER performance of IBM Models 1 and 2 using a couple of initialization methods with an HMM alignment model incorporating some heuristics for speed advantage. We demonstrate the effects of various symmetrization methods combining English-French and French-English alignments generated by all of these models, and discuss the efficacy of AER as a useful and informative metric and the alignment task in general. We report a minimum Alignment Error Rate of .126 tested on the first 1000 sentences of the Canadian Hansards corpus.
}

\section{Initializing IBM Models}

Given sets of English and French sentences $E_1^M$ and $F_1^N$, for every French token $f$, we designate an initial set $S_f$ of possible candidate translations as

\[
    S_f = \bigcup_{i=1}^N E_i \text{if} f \in F_i
\]

Under the naive scheme, probability is distributed among these candidates uniformly:

\[
    p(f|e) = \frac{1}{|S_f|}
\]

A more nuanced approach obtains initial weights by exploiting the string-level commonalities between English and French words that arise due to their languages' linguistic proximity. Where ld is the Levenshtein distance function, we take the {\em edit ratio} between to strings $a$ and $b$ to be 
\[
    \text{er}(a,b) = \frac{\text{ld}(a, b)}{\max(|a|, |b|)} + \delta
\]
and redefine the initial translation probabilities as
\[
    p(f|e) = \frac{\text{er}(f,e)}{\sum_{i=1}^{|S_f|} \text{er}(f, {S_f}_i)}
\]
The smoothing term $\delta$ serves as a flexible hyperparameter.  Larger $\delta$ approximated the more uniform initialization, while a small $\delta$ could potentially prevent source-token pairs form being translations of eachother. 

In development tuning, we found $\delta=1$ to yield optimal results, outperforming the uniform initialization baseline. In all further experiments, this method of initialization was used.

% Results are shown in Table \ref{tbl:ed_dist}.
 %todo more explain
% TODO cite msr paper as inspiration?

\begin{comment}
    \begin{table}[h]
\begin{center}
\begin{tabular}{l|ll}
    & \multicolumn{2}{c} {AER}\\
    initialization & Model 1 & Model 2\\ \hline
    uniform & & \\
    edit distance & &
\end{tabular}
\end{center}
\caption{Using a function of Levenshtein edit distance to seed Model 1 and Model 2 improves AER over the baseline uniform initialization.}
\label{tbl:ed_dist}
\end{table}
\end{comment}

\section{An HMM alignment model}

% rephrase this to be more specific
To test our intuition that local context ought to matter for alignment, we implemented a Hidden Markov Model-based alignment scheme ({\tt hmm-model.py})
following the work of 
~\cite{Vogel96hmm-basedword}.


In order to make the runtime of these experiments more manageable, we implemented a heuristic pruning of possible states for a each target token. The unpruned HMM latice includes all possible jumps from any word in the source. This makes the lattice beam width very large for longer sentences. To speed up computation we employed a pruning heuristic. Sentences that are less that 5 tokesn in length are left un-pruned. That is for each token in the target sentence, any of the 5 tokens in the source could be a state. For source sentences of lenght 5 to 15, the possible states that could generate a target token was reduced by 50\%. For each token in the target sentence, all possible states were sorted based on their initial translation probabilities. Only the top 50\% of these states were used as possile states in the HMM lattice. For longer sentences (length 15 to 35) only 30\% were used as possible states. For even longer sentences only top 10\% was used as possible states. This method of state space pruning is commonly used in many HMM-based models. For example, for POS tagging, the possible tags applicable for a word are pre-selected based on the word. 

We find that our HMM aligner gets better alignments from the ``reverse" direction, and improves as more iterations are run (Figure \ref{fig:hmm_learning_curve}).

\begin{figure}[ht]
\begin{center}
    \includegraphics[scale=.7]{res/hmm.eps}
\end{center}
\caption{
    We ran the HMM aligner for 10 iterations modelling $p(e|f)$ and found the best results after 8 iterations. In the ``forward" $p(f|e)$ direction, performance was worse and we only ran up to 4 iterations.
}
\label{fig:hmm_learning_curve}
\end{figure}

\section{Symmetrization of forward and reverse alignments}

Because many-to-one alignments are restricted to the F $\rightarrow$ E direction in the first two IBM models, we decided to try combining the output of the aligner when run in each direction in turn. One might hope that this process would go some way toward accomodating phrasal translations with a ``fertility" of less than 1, for example when an English idiom, phrase, or functional n-gram translates concisely as a single French word (Figure \ref{fig:etof}).

\begin{figure}[h]
\begin{center}
    \includegraphics[scale=.6]{fe_many}
    \includegraphics[scale=.6]{fe_many2}
\caption{
    In these excerpts from sentence 25 of the annotated Hansards corpus, we see evidence that makes the case for allowing many-to-one English-French alignments, which is impossible under pure Model 1.
}
\label{fig:etof}
\end{center}
\end{figure}

A simple approach to this task is described in ~\cite{koehn}, which we implement in ({\tt merge.py}).
Contrary to expectation, symmetrizing forward and reverse alignments in this way did not improve our measured alignment performance and indeed produced worse alignments than either individual direction alone (Table \ref{tab:summary}).
However, simply taking the intersection of the forward and reverse alignments did improve our score. This result furthers our skeptecism of the comprehensiveness and utility of the AER metric, as we discuss further in \ref{aer}.
% more detail about this?
\begin{table}[h]
\begin{center}
\begin{tabular}{r|rrrr}
    & $p(f|e)$ & $p(e|f)$ & sym & int\\ \hline
    Model 1 &0.357 & 0.316&0.371 & 0.230\\
    Model 2 &0.279& 0.245& 0.289& 0.171\\
    HMM - min iter &0.290 & 0.239& 0.296& 0.156\\
    HMM - max iter &0.259 &0.199 & 0.267& {\bf 0.126}\\
\end{tabular}
\end{center}
\caption{AER for each of our models in both directions, fully symmetrized with a growing heuristic, and simply intersected. Min-iter is 1 iteration of forward and 1 reverse; max-iter is the best result achieved for any number of iterations run, which occured at 4 iterations forward and 8 reversed (see Figure \ref{fig:hmm_learning_curve}).}
\label{tab:summary}
\end{table}



\section{AER and the Hansards alignments}
\label{aer}

We were surprised to discover that a tactic as simple as raw intersection could provide such a drastic and consistent improvement over any given unidirectional approach.
Inspecting the 37 provided manually annotated gold aligments, we observed that far more gold alignment links are Possible than are Sure. This indicates that there is a considerable amount of information deemed relevant by the annotators that isn't captured in the Sure alignments. However, systems evaluated with AER are penalized for straying too far away from the Sure alignments. If all the sure alignments are recovered and no spurious alignments proposed, it is impossible to improve AER by identifying any additional possible alignments. This seems to contradict the spirit of the value of Possible links evinced by their prevalence in the gold annotations.
The best way to improve AER appears to involve being as conservative as possible to avoid risking bad alignments, and therein lies the strength of the intersection method (Figure \ref{fig:links}).


\begin{figure}[h]
\begin{center}
    \includegraphics[scale=.6]{s10-gold}
    \includegraphics[scale=.6]{s10-all}
    \includegraphics[scale=.6]{s10-int}
\end{center}
\caption{
    Above: the gold alignment of the Hansards sentence 10, consisting mostly of Possible links. Middle: the combined (symmetrized) alignment produced by our best-performing HMM aligner including two bad spurious links. Bottom: intersected alignments from the same aligner covering all the gold Sure links, and additionally including some Possible links and no bad links. This alignment has exactly the same AER (0) as it would if it did not include the let-permettez and me-moi links, which would surely be much worse alignment.
}
\label{fig:links}
\end{figure}

\bibliography{writeup}{}
\bibliographystyle{plain}
\end{document}
